{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Non-English Tacotron 2 Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Zn56kLhTOp3-"
      },
      "outputs": [],
      "source": [
        "#@markdown # Check GPU type\n",
        "#@markdown ### Factory reset runtime if you don't have the desired GPU.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## It is recommended to not use the K80\n",
        "\n",
        "!nvidia-smi -L\n",
        "#@markdown All GPUs work properly, but vary in speed. K80 and P4 are not recommended.\n",
        "\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Anti-Disconnect for Google Colab\n",
        "#@markdown ## Run this to stop it from disconnecting automatically (will disconnect after 4+ hours, though.)\n",
        "\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "display(IPython.display.Javascript(js_code))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2Vo2pwpiOxHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Mount your Google Drive\n",
        "\n",
        "#Google Drive Authentication Token\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ceq1WuOMO1dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Download pretrained models and install tacotron 2\n",
        "\n",
        "#make wavs folder\n",
        "!mkdir '/content/wavs'\n",
        "\n",
        "#get ntk japanese pretrained model\n",
        "!gdown 1-5ULOICIs_BOndoqlVFB0BMjmvwiqGvE\n",
        "#get french pretrained model\n",
        "!gdown 1--lPwGhqFkqFZrd04Qhm90ndrepXifCf\n",
        "#get talqu pretrained model\n",
        "!gdown 1j986QrB1C-tY4GLq806xMBfMWVO3YKY8\n",
        "#get mandarin pretrained model\n",
        "!gdown 1lavjPjHtYAoe4qqOralsK9doCKIxd9s5\n",
        "\n",
        "#download tacotron 2\n",
        "!git clone -q https://github.com/NVIDIA/tacotron2\n",
        "!pip install unidecode\n",
        "!pip install tensorflow==1.15"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tgG4jm9MO6Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Optional: Unzip file to unpack wavs\n",
        "#@markdown ### If you have a lot of wav files, then zip them all into one file locally on your system, then upload it and copy the path. Otherwise, you may just upload your wavs to wavs/.\n",
        "#@markdown ---\n",
        "\n",
        "zip_file_path = \"/content/cleaned.zip\" #@param {type:\"string\"}\n",
        "!unzip $zip_file_path -d '/content/wavs'"
      ],
      "metadata": {
        "cellView": "form",
        "id": "N3tjz1mDQaI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Patch cleaners.py\n",
        "\n",
        "%%writefile /content/tacotron2/text/cleaners.py\n",
        "\"\"\" from https://github.com/keithito/tacotron \"\"\"\n",
        "\n",
        "'''\n",
        "Cleaners are transformations that run over the input text at both training and eval time.\n",
        "\n",
        "Cleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\"\n",
        "hyperparameter. Some cleaners are English-specific. You'll typically want to use:\n",
        "  1. \"english_cleaners\" for English text\n",
        "  2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using\n",
        "     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n",
        "  3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update\n",
        "     the symbols in symbols.py to match your data).\n",
        "'''\n",
        "\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from .numbers import normalize_numbers\n",
        "\n",
        "\n",
        "# Regular expression matching whitespace:\n",
        "_whitespace_re = re.compile(r'\\s+')\n",
        "\n",
        "# List of (regular expression, replacement) pairs for abbreviations:\n",
        "_abbreviations = [(re.compile('\\\\b%s\\\\.' % x[0], re.IGNORECASE), x[1]) for x in [\n",
        "  ('mrs', 'misess'),\n",
        "  ('mr', 'mister'),\n",
        "  ('dr', 'doctor'),\n",
        "  ('st', 'saint'),\n",
        "  ('co', 'company'),\n",
        "  ('jr', 'junior'),\n",
        "  ('maj', 'major'),\n",
        "  ('gen', 'general'),\n",
        "  ('drs', 'doctors'),\n",
        "  ('rev', 'reverend'),\n",
        "  ('lt', 'lieutenant'),\n",
        "  ('hon', 'honorable'),\n",
        "  ('sgt', 'sergeant'),\n",
        "  ('capt', 'captain'),\n",
        "  ('esq', 'esquire'),\n",
        "  ('ltd', 'limited'),\n",
        "  ('col', 'colonel'),\n",
        "  ('ft', 'fort'),\n",
        "]]\n",
        "\n",
        "\n",
        "def expand_abbreviations(text):\n",
        "  for regex, replacement in _abbreviations:\n",
        "    text = re.sub(regex, replacement, text)\n",
        "  return text\n",
        "\n",
        "\n",
        "def expand_numbers(text):\n",
        "  return normalize_numbers(text)\n",
        "\n",
        "\n",
        "def lowercase(text):\n",
        "  return text.lower()\n",
        "\n",
        "\n",
        "def collapse_whitespace(text):\n",
        "  return re.sub(_whitespace_re, ' ', text)\n",
        "\n",
        "\n",
        "def convert_to_ascii(text):\n",
        "  return unidecode(text)\n",
        "\n",
        "\n",
        "def basic_cleaners(text):\n",
        "  '''Basic pipeline that lowercases and collapses whitespace without transliteration.'''\n",
        "  text = lowercase(text)\n",
        "  text = collapse_whitespace(text)\n",
        "  return text\n",
        "\n",
        "\n",
        "def transliteration_cleaners(text):\n",
        "  '''Pipeline for non-English text that transliterates to ASCII.'''\n",
        "  text = convert_to_ascii(text)\n",
        "  text = lowercase(text)\n",
        "  text = collapse_whitespace(text)\n",
        "  return text\n",
        "\n",
        "\n",
        "def english_cleaners(text):\n",
        "  '''Pipeline for English text, including number and abbreviation expansion.'''\n",
        "  text = convert_to_ascii(text)\n",
        "  text = lowercase(text)\n",
        "  text = expand_numbers(text)\n",
        "  text = expand_abbreviations(text)\n",
        "  text = collapse_whitespace(text)\n",
        "  return text\n",
        "\n",
        "def return_text(text):\n",
        "  return text"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9CiVprUPQtBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Patch symbols.py\n",
        "\n",
        "%%writefile /content/tacotron2/text/symbols.py\n",
        "\"\"\" from https://github.com/keithito/tacotron \"\"\"\n",
        "\n",
        "'''\n",
        "Defines the set of symbols used in text input to the model.\n",
        "\n",
        "The default is a set of ASCII characters that works well for English or text that has been run through Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details. '''\n",
        "from text import cmudict\n",
        "\n",
        "_pad        = '_'\n",
        "_punctuation = '!\\'(),.:;? «»'\n",
        "_special = '-'\n",
        "_letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
        "\n",
        "# Prepend \"@\" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n",
        "_arpabet = ['@' + s for s in cmudict.valid_symbols]\n",
        "\n",
        "# Export all symbols:\n",
        "symbols = [_pad] + list(_special) + list(_punctuation) + list(_letters) + _arpabet\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rK6RaXUSRh7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown This is for your training configuration (hparams.py)\n",
        "\n",
        "%%writefile /content/tacotron2/hparams.py\n",
        "import tensorflow as tf\n",
        "from text import symbols\n",
        "\n",
        "transcription='/content/list_ntk.txt'#@param {type:'string'}\n",
        "batchsize=2#@param {type: 'integer'}\n",
        "\n",
        "def create_hparams(hparams_string=None, verbose=False):\n",
        "    \"\"\"Create model hyperparameters. Parse nondefault from given string.\"\"\"\n",
        "\n",
        "    hparams = tf.contrib.training.HParams(\n",
        "        ################################\n",
        "        # Experiment Parameters        #\n",
        "        ################################\n",
        "        epochs=5000,\n",
        "        iters_per_checkpoint=100,\n",
        "        seed=1234,\n",
        "        dynamic_loss_scaling=True,\n",
        "        fp16_run=False,\n",
        "        distributed_run=False,\n",
        "        dist_backend=\"nccl\",\n",
        "        dist_url=\"tcp://localhost:54321\",\n",
        "        cudnn_enabled=True,\n",
        "        cudnn_benchmark=False,\n",
        "        ignore_layers=['embedding.weight'],\n",
        "\n",
        "        ################################\n",
        "        # Data Parameters             #\n",
        "        ################################\n",
        "        load_mel_from_disk=False,\n",
        "        training_files=transcription,\n",
        "        validation_files=transcription,\n",
        "        text_cleaners=['return_text'],\n",
        "\n",
        "        ################################\n",
        "        # Audio Parameters             #\n",
        "        ################################\n",
        "        max_wav_value=32768.0,\n",
        "        sampling_rate=22050,\n",
        "        filter_length=1024,\n",
        "        hop_length=256,\n",
        "        win_length=1024,\n",
        "        n_mel_channels=80,\n",
        "        mel_fmin=0.0,\n",
        "        mel_fmax=8000.0,\n",
        "\n",
        "        ################################\n",
        "        # Model Parameters             #\n",
        "        ################################\n",
        "        n_symbols=len(symbols),\n",
        "        symbols_embedding_dim=512,\n",
        "\n",
        "        # Encoder parameters\n",
        "        encoder_kernel_size=5,\n",
        "        encoder_n_convolutions=3,\n",
        "        encoder_embedding_dim=512,\n",
        "\n",
        "        # Decoder parameters\n",
        "        n_frames_per_step=1,  # currently only 1 is supported\n",
        "        decoder_rnn_dim=1024,\n",
        "        prenet_dim=256,\n",
        "        max_decoder_steps=1000,\n",
        "        gate_threshold=0.5,\n",
        "        p_attention_dropout=0.1,\n",
        "        p_decoder_dropout=0.1,\n",
        "\n",
        "        # Attention parameters\n",
        "        attention_rnn_dim=1024,\n",
        "        attention_dim=128,\n",
        "\n",
        "        # Location Layer parameters\n",
        "        attention_location_n_filters=32,\n",
        "        attention_location_kernel_size=31,\n",
        "\n",
        "        # Mel-post processing network parameters\n",
        "        postnet_embedding_dim=512,\n",
        "        postnet_kernel_size=5,\n",
        "        postnet_n_convolutions=5,\n",
        "\n",
        "        ################################\n",
        "        # Optimization Hyperparameters #\n",
        "        ################################\n",
        "        use_saved_learning_rate=False,\n",
        "        learning_rate=1e-3,\n",
        "        weight_decay=1e-6,\n",
        "        grad_clip_thresh=1.0,\n",
        "        batch_size=batchsize, #if you have the T4, set this to 14 or less\n",
        "        mask_padding=True  # set model's padded outputs to padded values\n",
        "    )\n",
        "\n",
        "    if hparams_string:\n",
        "        tf.logging.info('Parsing command line hparams: %s', hparams_string)\n",
        "        hparams.parse(hparams_string)\n",
        "\n",
        "    if verbose:\n",
        "        tf.logging.info('Final parsed hparams: %s', hparams.values())\n",
        "\n",
        "    return hparams"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2bDbdjyAQi6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Begin training\n",
        "#@markdown neuTalk_Japanese_pretrained.pt = neuTalk-formatted Japanese data \\\n",
        "#@markdown FlatBaseModel_frontVoiceIsAkitoTenohira_20210418.pt = TALQu-formatted Japanese data \\\n",
        "#@markdown neuTalk_French-IPA_pretrained.pt = French data transcribed in IPA \\\n",
        "#@markdown neuTalk_Mandarin_pretrained.pt = Mandarin data transcribed with the appropriate g2p\n",
        "\n",
        "warm_start_model = \"neuTalk_Japanese_pretrained.pt\" #@param [\"neuTalk_Japanese_pretrained.pt\", \"neuTalk_French-IPA_pretrained.pt\", \"FlatBaseModel_frontVoiceIsAkitoTenohira_20210418.pt\", \"neuTalk_Mandarin_pretrained.pt\"]\n",
        "output_directory = \"/content/drive/MyDrive/jsut_loanwords128\" #@param{type:'string'}\n",
        "\n",
        "!python /content/tacotron2/train.py --log_directory='/content/logs' -c $warm_start_model --warm_start --output_directory=$output_directory"
      ],
      "metadata": {
        "id": "UxVe0GJ9SFw1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}