{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y6NHr8qIC1dO"
      },
      "outputs": [],
      "source": [
        "#@markdown # **1** Check which GPU you've been allocated\n",
        "#@markdown ### Disconnect & delete the runtime if you haven't been allocated a desired GPU.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## It is recommended not to use a K80 or P4 GPU\n",
        "\n",
        "!nvidia-smi -L\n",
        "#@markdown All GPUs work, but each of them vary in speed. K80 and P4 GPUs are usable, but not recommended."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **2** Anti-Disconnect for Google Colab\n",
        "#@markdown ## Run this to stop it from disconnecting automatically (will disconnect after 4+ hours, though.)\n",
        "\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "display(IPython.display.Javascript(js_code))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VTxp8djtDTzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## **3** Mount Google Drive\n",
        "\n",
        "#Google Drive Authentication Token\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gMg_KQPyDZvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **4** Execute this step to install Tacotron 2 and dependencies\n",
        "#@markdown ---\n",
        "#@markdown Select which language your model will be trained in.\n",
        "!pip install tensorflow==1.15.2\n",
        "import os\n",
        "!git clone -q https://github.com/NVIDIA/tacotron2\n",
        "os.chdir('tacotron2')\n",
        "!git submodule init\n",
        "!git submodule update\n",
        "!pip install -q unidecode tensorboardX\n",
        "!apt-get install pv\n",
        "!apt-get install jq\n",
        "!wget https://raw.githubusercontent.com/tonikelope/megadown/master/megadown -O megadown.sh\n",
        "!chmod 755 megadown.sh\n",
        "\n",
        "%matplotlib inline\n",
        "import os\n",
        "if os.getcwd() != '/content/tacotron2':\n",
        "    os.chdir('tacotron2')\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "from numpy import finfo\n",
        "\n",
        "import torch\n",
        "from distributed import apply_gradient_allreduce\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from model import Tacotron2\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from loss_function import Tacotron2Loss\n",
        "from logger import Tacotron2Logger\n",
        "from hparams import create_hparams\n",
        " \n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import layers\n",
        "from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "from text import text_to_sequence\n",
        "from math import e\n",
        "#from tqdm import tqdm # Terminal\n",
        "#from tqdm import tqdm_notebook as tqdm # Legacy Notebook TQDM\n",
        "from tqdm.notebook import tqdm # Modern Notebook TQDM\n",
        "from distutils.dir_util import copy_tree\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def download_from_google_drive(file_id, file_name):\n",
        "  # download a file from the Google Drive link\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id={file_id}\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm={confirm_text}&id={file_id}\" -o {file_name}\n",
        "\n",
        "def create_mels():\n",
        "    print(\"Generating Mels\")\n",
        "    stft = layers.TacotronSTFT(\n",
        "                hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "                hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "                hparams.mel_fmax)\n",
        "    def save_mel(filename):\n",
        "        audio, sampling_rate = load_wav_to_torch(filename)\n",
        "        if sampling_rate != stft.sampling_rate:\n",
        "            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(filename, \n",
        "                sampling_rate, stft.sampling_rate))\n",
        "        audio_norm = audio / hparams.max_wav_value\n",
        "        audio_norm = audio_norm.unsqueeze(0)\n",
        "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "        melspec = stft.mel_spectrogram(audio_norm)\n",
        "        melspec = torch.squeeze(melspec, 0).cpu().numpy()\n",
        "        np.save(filename.replace('.wav', ''), melspec)\n",
        "\n",
        "    import glob\n",
        "    wavs = glob.glob('wavs/*.wav')\n",
        "    for i in tqdm(wavs):\n",
        "        save_mel(i)\n",
        "\n",
        "\n",
        "def reduce_tensor(tensor, n_gpus):\n",
        "    rt = tensor.clone()\n",
        "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
        "    rt /= n_gpus\n",
        "    return rt\n",
        "\n",
        "\n",
        "def init_distributed(hparams, n_gpus, rank, group_name):\n",
        "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
        "    print(\"Initializing Distributed\")\n",
        "\n",
        "    # Set cuda device so everything is done on the right GPU.\n",
        "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
        "\n",
        "    # Initialize distributed communication\n",
        "    dist.init_process_group(\n",
        "        backend=hparams.dist_backend, init_method=hparams.dist_url,\n",
        "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
        "\n",
        "    print(\"Done initializing distributed\")\n",
        "\n",
        "\n",
        "def prepare_dataloaders(hparams):\n",
        "    # Get data, data loaders and collate function ready\n",
        "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
        "    valset = TextMelLoader(hparams.validation_files, hparams)\n",
        "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        train_sampler = DistributedSampler(trainset)\n",
        "        shuffle = False\n",
        "    else:\n",
        "        train_sampler = None\n",
        "        shuffle = True\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=hparams.batch_size, pin_memory=False,\n",
        "                              drop_last=True, collate_fn=collate_fn)\n",
        "    return train_loader, valset, collate_fn\n",
        "\n",
        "\n",
        "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
        "    if rank == 0:\n",
        "        if not os.path.isdir(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "            os.chmod(output_directory, 0o775)\n",
        "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
        "    else:\n",
        "        logger = None\n",
        "    return logger\n",
        "\n",
        "\n",
        "def load_model(hparams):\n",
        "    model = Tacotron2(hparams).cuda()\n",
        "    if hparams.fp16_run:\n",
        "        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model_dict = checkpoint_dict['state_dict']\n",
        "    if len(ignore_layers) > 0:\n",
        "        model_dict = {k: v for k, v in model_dict.items()\n",
        "                      if k not in ignore_layers}\n",
        "        dummy_dict = model.state_dict()\n",
        "        dummy_dict.update(model_dict)\n",
        "        model_dict = dummy_dict\n",
        "    model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    learning_rate = checkpoint_dict['learning_rate']\n",
        "    iteration = checkpoint_dict['iteration']\n",
        "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
        "        checkpoint_path, iteration))\n",
        "    return model, optimizer, learning_rate, iteration\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
        "    import random\n",
        "    if random.random() > 0.85:\n",
        "        print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
        "            iteration, filepath))\n",
        "        try:\n",
        "            torch.save({'iteration': iteration,\n",
        "                    'state_dict': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'learning_rate': learning_rate}, filepath)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"interrupt received while saving, waiting for save to complete.\")\n",
        "            torch.save({'iteration': iteration,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'learning_rate': learning_rate}, filepath)\n",
        "        print(\"Model Saved\")\n",
        "\n",
        "def plot_alignment(alignment, info=None):\n",
        "    %matplotlib inline\n",
        "    fig, ax = plt.subplots(figsize=(int(alignment_graph_width/100), int(alignment_graph_height/100)))\n",
        "    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n",
        "                   interpolation='none')\n",
        "    ax.autoscale(enable=True, axis=\"y\", tight=True)\n",
        "    fig.colorbar(im, ax=ax)\n",
        "    xlabel = 'Decoder timestep'\n",
        "    if info is not None:\n",
        "        xlabel += '\\n\\n' + info\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel('Encoder timestep')\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "    plt.show()\n",
        "\n",
        "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
        "             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate):\n",
        "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
        "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
        "                                shuffle=False, batch_size=batch_size,\n",
        "                                pin_memory=False, collate_fn=collate_fn)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            if distributed_run:\n",
        "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_val_loss = loss.item()\n",
        "            val_loss += reduced_val_loss\n",
        "        val_loss = val_loss / (i + 1)\n",
        "\n",
        "    model.train()\n",
        "    if rank == 0:\n",
        "        print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)/60, learning_rate))\n",
        "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
        "        if hparams.show_alignments:\n",
        "            %matplotlib inline\n",
        "            _, mel_outputs, gate_outputs, alignments = y_pred\n",
        "            idx = random.randint(0, alignments.size(0) - 1)\n",
        "            plot_alignment(alignments[idx].data.cpu().numpy().T)\n",
        "\n",
        "def train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus,\n",
        "          rank, group_name, hparams, log_directory2):\n",
        "    \"\"\"Training and validation logging results to tensorboard and stdout\n",
        "\n",
        "    Params\n",
        "    ------\n",
        "    output_directory (string): directory to save checkpoints\n",
        "    log_directory (string) directory to save tensorboard logs\n",
        "    checkpoint_path(string): checkpoint path\n",
        "    n_gpus (int): number of gpus\n",
        "    rank (int): rank of current gpu\n",
        "    hparams (object): comma separated list of \"name=value\" pairs.\n",
        "    \"\"\"\n",
        "    if hparams.distributed_run:\n",
        "        init_distributed(hparams, n_gpus, rank, group_name)\n",
        "\n",
        "    torch.manual_seed(hparams.seed)\n",
        "    torch.cuda.manual_seed(hparams.seed)\n",
        "\n",
        "    model = load_model(hparams)\n",
        "    learning_rate = hparams.learning_rate\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                                 weight_decay=hparams.weight_decay)\n",
        "\n",
        "    if hparams.fp16_run:\n",
        "        from apex import amp\n",
        "        model, optimizer = amp.initialize(\n",
        "            model, optimizer, opt_level='O2')\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    criterion = Tacotron2Loss()\n",
        "\n",
        "    logger = prepare_directories_and_logger(\n",
        "        output_directory, log_directory, rank)\n",
        "\n",
        "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
        "\n",
        "    # Load checkpoint if one exists\n",
        "    iteration = 0\n",
        "    epoch_offset = 0\n",
        "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
        "        if warm_start:\n",
        "            model = warm_start_model(\n",
        "                checkpoint_path, model, hparams.ignore_layers)\n",
        "        else:\n",
        "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
        "                checkpoint_path, model, optimizer)\n",
        "            if hparams.use_saved_learning_rate:\n",
        "                learning_rate = _learning_rate\n",
        "            iteration += 1  # next iteration is iteration + 1\n",
        "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
        "    else:\n",
        "      os.path.isfile(\"/content/tacotron2/pretrained_model\")\n",
        "      pretrainedmodel = \"Japanese (neuTalk)\" #@param [\"French\", \"Japanese (neuTalk)\", \"Japanese (TALQu)\", \"Mandarin\"]\n",
        "      if pretrainedmodel=='French':\n",
        "        !gdown -O pretrained_model 1--lPwGhqFkqFZrd04Qhm90ndrepXifCf\n",
        "      elif pretrainedmodel=='Japanese (neuTalk)':\n",
        "        !gdown -O pretrained_model 1-5ULOICIs_BOndoqlVFB0BMjmvwiqGvE\n",
        "      elif pretrainedmodel=='Japanese (TALQu)':\n",
        "        !wget -O pretrained_model https://github.com/Haruqa/tacotron2/releases/download/20210429/FlatBaseModel_frontVoiceIsAkitoTenohira_20210418.pt\n",
        "      elif pretrainedmodel=='Mandarin':\n",
        "        !gdown -O pretrained_model 1lavjPjHtYAoe4qqOralsK9doCKIxd9s5\n",
        "      else:\n",
        "        print(\"Invalid pretrained model selection!\")\n",
        "      %cd /content/tacotron2\n",
        "      model = warm_start_model(\"/content/tacotron2/pretrained_model\", model, hparams.ignore_layers)\n",
        "      # download LJSpeech pretrained model if no checkpoint already exists\n",
        "    \n",
        "    start_eposh = time.perf_counter()\n",
        "    learning_rate = 0.0\n",
        "    model.train()\n",
        "    is_overflow = False\n",
        "    # ================ MAIN TRAINNIG LOOP! ===================\n",
        "    for epoch in tqdm(range(epoch_offset, hparams.epochs)):\n",
        "        print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n",
        "        start_eposh = time.perf_counter() # eposh is russian, not a typo\n",
        "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "            start = time.perf_counter()\n",
        "            if iteration < hparams.decay_start: learning_rate = hparams.A_\n",
        "            else: iteration_adjusted = iteration - hparams.decay_start; learning_rate = (hparams.A_*(e**(-iteration_adjusted/hparams.B_))) + hparams.C_\n",
        "            learning_rate = max(hparams.min_learning_rate, learning_rate) # output the largest number\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = learning_rate\n",
        "\n",
        "            model.zero_grad()\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            if hparams.distributed_run:\n",
        "                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_loss = loss.item()\n",
        "            if hparams.fp16_run:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            if hparams.fp16_run:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    amp.master_params(optimizer), hparams.grad_clip_thresh)\n",
        "                is_overflow = math.isnan(grad_norm)\n",
        "            else:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    model.parameters(), hparams.grad_clip_thresh)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if not is_overflow and rank == 0:\n",
        "                duration = time.perf_counter() - start\n",
        "                logger.log_training(\n",
        "                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
        "                #print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n",
        "\n",
        "            iteration += 1\n",
        "        validate(model, criterion, valset, iteration,\n",
        "                 hparams.batch_size, n_gpus, collate_fn, logger,\n",
        "                 hparams.distributed_run, rank, epoch, start_eposh, learning_rate)\n",
        "        save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n",
        "        if log_directory2 != None:\n",
        "            copy_tree(log_directory, log_directory2)\n",
        "def check_dataset(hparams):\n",
        "    from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "    import os\n",
        "    import numpy as np\n",
        "    def check_arr(filelist_arr):\n",
        "        for i, file in enumerate(filelist_arr):\n",
        "            if len(file) > 2:\n",
        "                print(\"|\".join(file), \"\\nhas multiple '|', this may not be an error.\")\n",
        "            if hparams.load_mel_from_disk and '.wav' in file[0]:\n",
        "                print(\"[WARNING]\", file[0], \" in filelist while expecting .npy .\")\n",
        "            else:\n",
        "                if not hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                    print(\"[WARNING]\", file[0], \" in filelist while expecting .wav .\")\n",
        "            if (not os.path.exists(file[0])):\n",
        "                print(\"|\".join(file), \"\\n[WARNING] does not exist.\")\n",
        "            if len(file[1]) < 3:\n",
        "                print(\"|\".join(file), \"\\n[info] has no/very little text.\")\n",
        "            if not ((file[1].strip())[-1] in r\"!?,.;:\"):\n",
        "                print(\"|\".join(file), \"\\n[info] has no ending punctuation.\")\n",
        "            mel_length = 1\n",
        "            if hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                melspec = torch.from_numpy(np.load(file[0], allow_pickle=True))\n",
        "                mel_length = melspec.shape[1]\n",
        "            if mel_length == 0:\n",
        "                print(\"|\".join(file), \"\\n[WARNING] has 0 duration.\")\n",
        "    print(\"Checking Training Files\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.training_files) # get split lines from training_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Checking Validation Files\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.validation_files) # get split lines from validation_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Finished Checking\")\n",
        "\n",
        "warm_start=False#sorry bout that\n",
        "n_gpus=1\n",
        "rank=0\n",
        "group_name=None\n",
        "\n",
        "# ---- DEFAULT PARAMETERS DEFINED HERE ----\n",
        "hparams = create_hparams()\n",
        "model_filename = 'current_model'\n",
        "hparams.training_files = \"filelists/clipper_train_filelist.txt\"\n",
        "hparams.validation_files = \"filelists/clipper_val_filelist.txt\"\n",
        "#hparams.use_mmi=True,          # not used in this notebook\n",
        "#hparams.use_gaf=True,          # not used in this notebook\n",
        "#hparams.max_gaf=0.5,           # not used in this notebook\n",
        "#hparams.drop_frame_rate = 0.2  # not used in this notebook\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "hparams.decay_start = 15000\n",
        "hparams.A_ = 5e-4\n",
        "hparams.B_ = 8000\n",
        "hparams.C_ = 0\n",
        "hparams.min_learning_rate = 1e-5\n",
        "generate_mels = True\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "hparams.batch_size = 32\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.ignore_layers = []\n",
        "hparams.epochs = 10000\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "output_directory = '/content/drive/My Drive/colab/outdir' # Location to save Checkpoints\n",
        "log_directory = '/content/tacotron2/logs' # Location to save Log files locally\n",
        "log_directory2 = '/content/drive/My Drive/colab/logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
        "checkpoint_path = output_directory+(r'/')+model_filename\n",
        "\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "!sed -i -- 's,.wav|,.npy|,g' filelists/*.txt\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "\n",
        "%cd /content/tacotron2\n",
        "\n",
        "data_path = 'wavs'\n",
        "!mkdir {data_path}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rJwhEnN9Ddsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Optional: Unzip file to unpack wavs\n",
        "#@markdown ### If you have a lot of wav files, then zip them all into one file locally on your system, then upload it and copy the path. Otherwise, you may just upload your wavs to tacotron2/wavs.\n",
        "#@markdown ---\n",
        "\n",
        "zip_file_path = \"/content/cleaned.zip\" #@param {type:\"string\"}\n",
        "!unzip $zip_file_path -d '/content/tacotron2/wavs'"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yoxUMeGzDqIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **5** Patch cleaners.py\n",
        "\n",
        "%%writefile /content/tacotron2/text/cleaners.py\n",
        "\"\"\" from https://github.com/keithito/tacotron \"\"\"\n",
        "\n",
        "'''\n",
        "Cleaners are transformations that run over the input text at both training and eval time.\n",
        "\n",
        "Cleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\"\n",
        "hyperparameter. Some cleaners are English-specific. You'll typically want to use:\n",
        "  1. \"english_cleaners\" for English text\n",
        "  2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using\n",
        "     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n",
        "  3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update\n",
        "     the symbols in symbols.py to match your data).\n",
        "'''\n",
        "\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from .numbers import normalize_numbers\n",
        "\n",
        "\n",
        "# Regular expression matching whitespace:\n",
        "_whitespace_re = re.compile(r'\\s+')\n",
        "\n",
        "# List of (regular expression, replacement) pairs for abbreviations:\n",
        "_abbreviations = [(re.compile('\\\\b%s\\\\.' % x[0], re.IGNORECASE), x[1]) for x in [\n",
        "  ('mrs', 'misess'),\n",
        "  ('mr', 'mister'),\n",
        "  ('dr', 'doctor'),\n",
        "  ('st', 'saint'),\n",
        "  ('co', 'company'),\n",
        "  ('jr', 'junior'),\n",
        "  ('maj', 'major'),\n",
        "  ('gen', 'general'),\n",
        "  ('drs', 'doctors'),\n",
        "  ('rev', 'reverend'),\n",
        "  ('lt', 'lieutenant'),\n",
        "  ('hon', 'honorable'),\n",
        "  ('sgt', 'sergeant'),\n",
        "  ('capt', 'captain'),\n",
        "  ('esq', 'esquire'),\n",
        "  ('ltd', 'limited'),\n",
        "  ('col', 'colonel'),\n",
        "  ('ft', 'fort'),\n",
        "]]\n",
        "\n",
        "\n",
        "def expand_abbreviations(text):\n",
        "  for regex, replacement in _abbreviations:\n",
        "    text = re.sub(regex, replacement, text)\n",
        "  return text\n",
        "\n",
        "\n",
        "def expand_numbers(text):\n",
        "  return normalize_numbers(text)\n",
        "\n",
        "\n",
        "def lowercase(text):\n",
        "  return text.lower()\n",
        "\n",
        "\n",
        "def collapse_whitespace(text):\n",
        "  return re.sub(_whitespace_re, ' ', text)\n",
        "\n",
        "\n",
        "def convert_to_ascii(text):\n",
        "  return unidecode(text)\n",
        "\n",
        "\n",
        "def basic_cleaners(text):\n",
        "  '''Basic pipeline that lowercases and collapses whitespace without transliteration.'''\n",
        "  text = lowercase(text)\n",
        "  text = collapse_whitespace(text)\n",
        "  return text\n",
        "\n",
        "\n",
        "def transliteration_cleaners(text):\n",
        "  '''Pipeline for non-English text that transliterates to ASCII.'''\n",
        "  text = convert_to_ascii(text)\n",
        "  text = lowercase(text)\n",
        "  text = collapse_whitespace(text)\n",
        "  return text\n",
        "\n",
        "\n",
        "def english_cleaners(text):\n",
        "  '''Pipeline for English text, including number and abbreviation expansion.'''\n",
        "  text = convert_to_ascii(text)\n",
        "  text = lowercase(text)\n",
        "  text = expand_numbers(text)\n",
        "  text = expand_abbreviations(text)\n",
        "  text = collapse_whitespace(text)\n",
        "  return text\n",
        "\n",
        "def return_text(text):\n",
        "  return text"
      ],
      "metadata": {
        "cellView": "form",
        "id": "o3p4A1wsD5y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **6** Patch symbols.py\n",
        "\n",
        "%%writefile /content/tacotron2/text/symbols.py\n",
        "\"\"\" from https://github.com/keithito/tacotron \"\"\"\n",
        "\n",
        "'''\n",
        "Defines the set of symbols used in text input to the model.\n",
        "\n",
        "The default is a set of ASCII characters that works well for English or text that has been run through Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details. '''\n",
        "from text import cmudict\n",
        "\n",
        "_pad        = '_'\n",
        "_punctuation = '!\\'(),.:;? «»'\n",
        "_special = '-'\n",
        "_letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
        "\n",
        "# Prepend \"@\" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n",
        "_arpabet = ['@' + s for s in cmudict.valid_symbols]\n",
        "\n",
        "# Export all symbols:\n",
        "symbols = [_pad] + list(_special) + list(_punctuation) + list(_letters) + _arpabet\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZhwuIThrD_Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **7** Patch hparams.py\n",
        "%%writefile /content/tacotron2/hparams.py\n",
        "import tensorflow as tf\n",
        "from text import symbols\n",
        "\n",
        "\n",
        "def create_hparams(hparams_string=None, verbose=False):\n",
        "    \"\"\"Create model hyperparameters. Parse nondefault from given string.\"\"\"\n",
        "\n",
        "    hparams = tf.contrib.training.HParams(\n",
        "        ################################\n",
        "        # Experiment Parameters        #\n",
        "        ################################\n",
        "        epochs=500,\n",
        "        iters_per_checkpoint=1000,\n",
        "        seed=1234,\n",
        "        dynamic_loss_scaling=True,\n",
        "        fp16_run=False,\n",
        "        distributed_run=False,\n",
        "        dist_backend=\"nccl\",\n",
        "        dist_url=\"tcp://localhost:54321\",\n",
        "        cudnn_enabled=True,\n",
        "        cudnn_benchmark=False,\n",
        "        ignore_layers=[],\n",
        "\n",
        "        ################################\n",
        "        # Data Parameters             #\n",
        "        ################################\n",
        "        load_mel_from_disk=False,\n",
        "        training_files='filelists/ljs_audio_text_train_filelist.txt',\n",
        "        validation_files='filelists/ljs_audio_text_val_filelist.txt',\n",
        "        text_cleaners=['return_text'],\n",
        "\n",
        "        ################################\n",
        "        # Audio Parameters             #\n",
        "        ################################\n",
        "        max_wav_value=32768.0,\n",
        "        sampling_rate=22050,\n",
        "        filter_length=1024,\n",
        "        hop_length=256,\n",
        "        win_length=1024,\n",
        "        n_mel_channels=80,\n",
        "        mel_fmin=0.0,\n",
        "        mel_fmax=8000.0,\n",
        "\n",
        "        ################################\n",
        "        # Model Parameters             #\n",
        "        ################################\n",
        "        n_symbols=len(symbols),\n",
        "        symbols_embedding_dim=512,\n",
        "\n",
        "        # Encoder parameters\n",
        "        encoder_kernel_size=5,\n",
        "        encoder_n_convolutions=3,\n",
        "        encoder_embedding_dim=512,\n",
        "\n",
        "        # Decoder parameters\n",
        "        n_frames_per_step=1,  # currently only 1 is supported\n",
        "        decoder_rnn_dim=1024,\n",
        "        prenet_dim=256,\n",
        "        max_decoder_steps=1000,\n",
        "        gate_threshold=0.5,\n",
        "        p_attention_dropout=0.1,\n",
        "        p_decoder_dropout=0.1,\n",
        "\n",
        "        # Attention parameters\n",
        "        attention_rnn_dim=1024,\n",
        "        attention_dim=128,\n",
        "\n",
        "        # Location Layer parameters\n",
        "        attention_location_n_filters=32,\n",
        "        attention_location_kernel_size=31,\n",
        "\n",
        "        # Mel-post processing network parameters\n",
        "        postnet_embedding_dim=512,\n",
        "        postnet_kernel_size=5,\n",
        "        postnet_n_convolutions=5,\n",
        "\n",
        "        ################################\n",
        "        # Optimization Hyperparameters #\n",
        "        ################################\n",
        "        use_saved_learning_rate=False,\n",
        "        learning_rate=1e-3,\n",
        "        weight_decay=1e-6,\n",
        "        grad_clip_thresh=1.0,\n",
        "        batch_size=64,\n",
        "        mask_padding=True  # set model's padded outputs to padded values\n",
        "    )\n",
        "\n",
        "    if hparams_string:\n",
        "        tf.logging.info('Parsing command line hparams: %s', hparams_string)\n",
        "        hparams.parse(hparams_string)\n",
        "\n",
        "    if verbose:\n",
        "        tf.logging.info('Final parsed hparams: %s', hparams.values())\n",
        "\n",
        "    return hparams\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BoMJMnkgOIFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## **8** Setting the model parameters\n",
        "#@markdown ---\n",
        "#@markdown The name of the TT2 model\n",
        "model_filename = 'jvs002' #@param {type: \"string\"}\n",
        "#@markdown ---\n",
        "#@markdown Upload your transcription and then copy+paste the path here.\n",
        "Training_file = \"/content/list_ntk.txt\" #@param {type: \"string\"}\n",
        "hparams.training_files = Training_file\n",
        "hparams.validation_files = Training_file\n",
        "#@markdown ---\n",
        "# hparams to Tune\n",
        "#hparams.use_mmi=True,          # not used in this notebook\n",
        "#hparams.use_gaf=True,          # not used in this notebook\n",
        "#hparams.max_gaf=0.5,           # not used in this notebook\n",
        "#hparams.drop_frame_rate = 0.2  # not used in this notebook\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "\n",
        "# Learning Rate             # https://www.desmos.com/calculator/ptgcz4vzsw / https://cdn.discordapp.com/attachments/841461499946860576/993611452499902504/scrnli_7_4_2022_1-16-48_PM.png\n",
        "hparams.decay_start = 15000         # wait till decay_start to start decaying learning rate\n",
        "hparams.B_ = 8000                   # Decay Rate\n",
        "hparams.C_ = 0                      # Shift learning rate equation by this value\n",
        "\n",
        "# Quality of Life\n",
        "generate_mels = True\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "\n",
        "hparams.epochs = 5000\n",
        "\n",
        "#@markdown The batch size. Lower if you don't have enough RAM.\n",
        "#@markdown If the GPU you've been allocated is a T4, use a batch size that is no larger than 14, and if the GPU you've been allocated is a P100, use a batch size that is no larger than 32.\n",
        "hparams.batch_size = 2 #@param {type: \"integer\"}\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.ignore_layers = [] # Layers to reset (None by default, other than foreign languages this param can be ignored)\n",
        "#@markdown ---\n",
        "#@markdown #### The learning rate and the minimum learning rate\n",
        "hparams.learning_rate = 1e-3 #@param\n",
        "hparams.min_learning_rate = 1e-5 #@param\n",
        "\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "#@markdown ---\n",
        "#@markdown #### Where to save the model when training\n",
        "output_directory = '/content/drive/MyDrive/jvs002_new' #@param {type: \"string\"}\n",
        "log_directory = '/content/tacotron2/logs' # Location to save Log files locally\n",
        "log_directory2 = '/content/drive/MyDrive/Tacotron 2/logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
        "checkpoint_path = output_directory+(r'/')+model_filename"
      ],
      "metadata": {
        "cellView": "form",
        "id": "78AyDBKTEJZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **9** Convert the wavs into mel spectrograms\n",
        "#@markdown #### This cell also checks for missing files\n",
        "print(\"Generating mels\")\n",
        "if generate_mels:\n",
        "    create_mels()\n",
        "\n",
        "print(\"Checking for missing files\")\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}; sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n",
        "\n",
        "check_dataset(hparams)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fWdMfOvvESqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **10** Train the model\n",
        "\n",
        "print('FP16 Run:', hparams.fp16_run)\n",
        "print('Dynamic Loss Scaling:', hparams.dynamic_loss_scaling)\n",
        "print('Distributed Run:', hparams.distributed_run)\n",
        "print('cuDNN Enabled:', hparams.cudnn_enabled)\n",
        "print('cuDNN Benchmark:', hparams.cudnn_benchmark)\n",
        "train(output_directory, log_directory, checkpoint_path,\n",
        "      warm_start, n_gpus, rank, group_name, hparams, log_directory2)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7B0ziRi0EXA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}